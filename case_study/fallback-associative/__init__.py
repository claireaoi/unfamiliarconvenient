# !/usr/local/bin/python3
# -*- coding: utf-8 -*-

######Description############
#
# TBA
#
######About############
#
# TBA
#
#**************************************LIBRARY IMPORT************
from mycroft.messagebus.message import Message
import numpy as np
import random
import re
import json
import time
import urllib.request
import os.path 
from os import path
import pathlib
import operator
import nltk
from nltk import word_tokenize, sent_tokenize, pos_tag
from nltk.corpus import words, wordnet
from nltk.stem.wordnet import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import wikipediaapi #TODO:Better use wikipedia?
wikipedia = wikipediaapi.Wikipedia('en')
import requests
from bs4 import BeautifulSoup #More beautiful to see html
import urllib.parse
import lxml
import cssselect
import sys
import subprocess 
# from googleapiclient.discovery import build #METHOD 1 with BUILD and google_search function for previous scraper
import spacy
from string import punctuation
from googlesearch import search
import newspaper
from urllib.error import URLError
########mycroft imports
from mycroft.audio import wait_while_speaking
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from mycroft.filesystem import FileSystemAccess

from .utils import hatchSelf, load_data_txt, load_data_json, cut_extract, crop_unfinished_sentence, semanticSimilarity

#*************************************************INTERACTION PARAMETERS******************************************

#TODO: EVOLVE both With size graph: nSelf=len(list(self.graph.keys()))
#TODO: gpt2 Training ?

global config
config={
    "gpt2_temperature": 1.0,
    "gpt2_temperature_variance": 0.3,
    "threshold_similarity": 0.1,# threshold when to consider 2 concepts as similar
    "num_sim_concept": 30,# when compare for words in self, this is a max number look for, else slow down too much
    "min_char_save": 40, #minumun of character to save for later train gpt2...
    "gpt2_context_length": 80, #max length of context to seed gpt2. ACtually, would be even double than this de facto
    "opinion_length": 500,#length opinion generated by gpt2
    "opinion_length_variance": 40, #vairnace of the length of that opinion
    "max_pick_word": 20,
    "scraping_min_length": 66, #min char where think is good length
    "scraping_max_length": 200 #max length of extract will share #NOTE: May be at 50 to test...
}

#***********************************************************************GLOBAL PARAMETERS***************************************************************************

global FIRST_RUN
FIRST_RUN=False #NOTE: Should be True on First run only
global path_ML_model
path_ML_model=str(pathlib.Path(__file__).parent.absolute())+'/gpt-2'
global SAVE_BLA
SAVE_BLA=True
global OWN_ML_MODEL
OWN_ML_MODEL=False
HEARD_HUMAN_FILE='heard_human.txt'
DATA_FILE="selfdata.json"
GRAPH_FILE="graph.json"
HEARD_ONLINE_FILE="heard_online.txt"
EXCLUDED=['a', 'the', 'an', 'I', 'to', 'are', 'not', 'for', 'best','you', 'they', 'she', 'he', 'if', 'me', 'on', 'is', 'are', 'them', 'why', 'per', 'out', 'with', 'by'] #exclude these words to be looked upon


#***********************************************************************MAIN CLASS***************************************************************************

from mycroft.skills.core import FallbackSkill

class AssociativeFallback(FallbackSkill):
    """
        A Fallback skill running some associative self quest, mapping the world
    """
    def __init__(self):
        super(AssociativeFallback, self).__init__()
        self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.log.info("(001) Loading GPT2...")
        if OWN_ML_MODEL:
            self.model=GPT2LMHeadModel.from_pretrained(path_ML_model)
        else:
            self.model=GPT2LMHeadModel.from_pretrained("gpt2")
        self.log.info("(002) Loading Self...")
        self.data, self.graph=dict(),dict()
        self.load_self(FIRST_RUN, config["max_pick_word"], config["threshold_similarity"])
        self.human_opinion=None
        #NOTE: Could USE SKILL SETTINGS ? skill settings are also a good option if you are saving non binary data, skill settings are basically a persistent dict saved as a .json file
        self.MSG_ASKOPINION, self.MSG_COMETOGETHER, self.MSG_COMETOGETHER3, self.MSG_INTEREST, self.MSG_LETMETHINK, self.MSG_NOINTEREST, self.MSG_SHARE, self.MSG_NOTED=[], [], [],[], [], [],[], []
        self.load_messages()
        self.log.info("(003) Good to go!")

    def initialize(self):
        """
            Registers the fallback handler.
            The second Argument is the priority associated to the request.
            Lower is higher priority. But number 1-4 are bypassing other skills.
            Can register several handle
        """
        #self.reload_skill=False#avoid autoreload 
        self.register_fallback(self.handle_associative, 6)#1 means always trigger it here
    
#***********************************************************************************************************************************************
#**********************************************************************MAIN INTERACTION BELOW*************************************************************************
#***********************************************************************************************************************************************

    def handle_associative(self, message):
        """
            First part of the interaction
        """
        concepts, extracts=[], [] #reinitialise
        no_new_concept, if_added_concept=False, False

        #(0) Get the human utterance
        utterance = message.data.get("utterance")
        self.log.info(f'Caught Human Bla: "{utterance}"')
        human_bla =str(utterance)
        if SAVE_BLA and len(human_bla)>config["min_char_save"]:
            saved_data=human_bla + "\n"
            self.save_data_file(HEARD_HUMAN_FILE, saved_data, mode="a")
            self.log.info("Saved Human bla.")
    
        self.log.info("=======================================================")
        self.log.info("------------------INTERACT PART 1------------------")
        self.log.info("=======================================================")
        
         #(1) Chris update its lifetime, save it and ask the listener to be patient.
        self.log.info("=======================================================")
        self.log.info("Step 1--Preliminaries")
        self.log.info("=======================================================")
        #self.speak(random.choice(self.MSG_LETMETHINK))# discarded for now
        self.data["lifetime"]+=1

        #(2) Chris extract one or two word from utterance
        self.log.info("=======================================================")
        self.log.info("Step 2- Extract words from human Bla") 
        self.log.info("=======================================================")
        ## OKWords, self.graph=extractWiki(human_bla, self.graph, self.data["memory"], MAX_PICK_WORD) #TODO Check ALTERNATIVE EXTRACTION? 
        OKWords = self.extract(human_bla, config["max_pick_word"])
        self.log.info("Words extracted from human blabla with wordnet:"+",".join(OKWords))

        #(3) Pick one word from this list (if not empty), and look for similar concept in self
        self.log.info("=======================================================")
        self.log.info("Step 3--Look for a similar self-concept")
        self.log.info("=======================================================") 
        if OKWords==[]:
            no_new_concept=True
            self.log.info("Did not hear any new word interesting in human blabla.")
        else:
            new_concept=random.choice(OKWords)
            self.log.info("Picked a new concept:"+new_concept)
            if_added_concept, closer_concept=self.isSelf(new_concept, config["num_sim_concept"],config["threshold_similarity"])
            self.log.info("Closer concept in self-graph {}. If added it to self graph: {}".format(closer_concept, if_added_concept))
            self.data["memory"].append(new_concept)#update memory and save
            self.save_data_json(DATA_FILE, self.data, mode="w")
      
            if if_added_concept:
                concepts=[new_concept,closer_concept]
                self.save_data_json(GRAPH_FILE, self.graph, mode="w")
                #self.speak(random.choice(self.MSG_INTEREST))#NOTE: discarded for now
            else:
                self.log.info("Did not find a concept similar enough.")

        # has not find a new concept interesting him. Will Look about two self concepts online. or one ?
        if no_new_concept or (not if_added_concept):
            #NOTE: Here end the interaction
            self.speak(random.choice(self.MSG_NOINTEREST))
            # concepts=self.graph.keys()#his self-graph
            # concepts=[random.choice(list(concepts)),random.choice(list(concepts))] #Or pick last?
            # while concepts[0]==concepts[1]:
            #     concepts[1]=random.choice(list(concepts))
            return True
        
        else:
            self.log.info("=======================================================")
            self.log.info("Step 4--Surf online space and Scrap")
            self.log.info("=======================================================")
            #(4) ONLINE SURF AND SCRAP 
            query= concepts[0]+ " "+ concepts[1]
            come_together=random.choice(self.MSG_COMETOGETHER)
            come_together=come_together.replace("xxx",concepts[0])
            come_together=come_together.replace("yyy",concepts[1])
            self.speak(come_together)
            self.log.info(come_together+ "Now surfing the online space...")
            #reset time to avoid query timeout
            #self.bus.emit(message.response({"phrase":"Chill, I am scraping.", "skill_id":self.skill_id, "searching":True}))#searxch_phrase
            #self.bus.emit(Message('question:query.response', data={"phrase":message.data.get('utterance'), "skill_id":self.skill_id, "searching":True}))
            scraped_data, extract_surf = self.surf_google(query, config["scraping_max_length"])
            # saving data scraped
            self.log.info("Saving scraped data in"+HEARD_ONLINE_FILE)
            self.save_data_file(HEARD_ONLINE_FILE, scraped_data+"\n\n", mode="a")#"\n".join(scraped_data)

            #NOTE Temporary, to check:
            self.log.info("scraped data"+scraped_data[:40])
            self.log.info("extract surf"+extract_surf[:40])

            # (5) Say a bit of the article about what found online
            self.log.info("=======================================================")
            self.log.info("step 5---Share what found:")
            self.log.info("=======================================================")
            self.speak(random.choice(self.MSG_SHARE))
            self.log.info(extract_surf)
            self.speak(extract_surf)

            wait_while_speaking()

            self.log.info("=======================================================")
            self.log.info("-----step 6--------Ask for Human Opinion-----------------")
            self.log.info("=======================================================")
            #self.bus.emit(Message('question:query.response', data={"phrase":message.data.get('utterance'), "skill_id":self.skill_id, "searching":True}))
           
            human_opinion=self.get_response('ask.opinion', num_retries=0)   #data=None, validator=None,on_fail=None, num_retries=-1 CHECK PARAM GET RESPONSE
            self.log.info("Just asked for opinion")        
            
            if human_opinion is not None:#do not need both, check new utterance not same than old one...
                self.log.info("=======================================================")
                self.log.info("============YOUHOUUUU CAUGHT HUMAN OPINION===========")
                self.log.info("=======================================================")
                human_opinion = str(human_opinion)
                self.log.info("Current human opinion saved:{}".format(human_opinion))
                #self.speak(random.choice(self.MSG_NOTED)) #discarded
                if SAVE_BLA and len(human_opinion)>config["min_char_save"]:
                    self.save_data_file(HEARD_HUMAN_FILE, human_opinion+"\n", mode="a")
                        
                self.log.info("=======================================================")
                self.log.info("step 7----Give his own opinion")
                self.log.info("=======================================================")
                #(7) Give his own opinion: generate it with gpt-2 after mix of online text and human opinion
                OPINION=["Yet, in my opinion", "Yet, I think", "Although I think", "I tend to think that","I feel", "Personally,I feel"]
                #---build a context for the gpt2 generation
                seed=extract_surf[-config["gpt2_context_length"]:]+ " " + human_opinion[-config["gpt2_context_length"]:]
                context= seed+"\n"+random.choice(OPINION)
                self.log.info("Context to seed gpt2 opinion:"+context)
                #--param for gpt2 generation
                length_output=config["opinion_length"] + random.randint(-config["opinion_length_variance"], +config["opinion_length_variance"])
                temperature_gpt2=round(config["gpt2_temperature"]+config["gpt2_temperature_variance"]*random.uniform(-1,1),1) #round 1 or 2 after ,
                #---gpt2 generation
                opinion = self.gpt2_text_generation(context, length_output, temperature_gpt2) 
                self.log.info("full generation:"+opinion)#NOTE:temporary to see if line below ok
                opinion = opinion.replace(seed, "")
                self.speak(opinion)
                self.log.info("Opinion it generated after replace:"+opinion)
                self.log.info("Thanks for this interaction.")
                
            else:
                self.speak("I'll take your silence as a yes")
                self.log.info("Seems human opinion is void.")

            return True #IF HANDLED...

#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#**********************************************************************SELF QUEST PROCEDURE *************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************

    def load_self(self, first_run, max_pick_word, threshold_similarity):
        """
            The VA loads his self.graph, memory, lifetime, as last saved. Or build it if first time.
        """
        if first_run:
            self.log.info("Hatching self in process...")
            self.graph, self.data, description=hatchSelf(max_pick_word, threshold_similarity)
        else:
            self.log.info("Loading self in process...")
            self.graph=load_data_json(GRAPH_FILE)
            self.data=load_data_json(DATA_FILE)
            self.log.info("I am here. My lifetime is {} interactions".format(self.data["lifetime"]))
            #self.log.info("selfgraph {}".format(self.graph.keys()))

    def isSelf(self, word, num_sim_concept, threshold_similarity):
        """
        Check if a word (not belonging to his self) is related to his self.graph.
        And pick a similar concept (any above the threshold of similarity).
        """
        nSelf=len(list(self.graph.keys()))
        #CASE in case graph becomes too big:
        indices=random.sample(range(0, nSelf), min(num_sim_concept, nSelf)) #Generate random list of indices where will look for
        self.graph[word]=[0,dict()]   #Add entry to dictionary for now
        ifConnected=False
        maxSim=0
        possible_simWord=[]
        simWord=""
        #Check similarity with other concepts in Self
        for i, wordSelf in enumerate(list(self.graph.keys())):
            if i in indices:
                similarity_score= semanticSimilarity(word,wordSelf)
                if similarity_score>threshold_similarity:
                    possible_simWord.append(wordSelf)
                    self.graph[word][1][wordSelf]=similarity_score#Add a connection if related enough.
                    self.graph[wordSelf][1][word]=similarity_score#Symmetric
                    ifConnected=True
                    #if similarity_score>maxSim:#IF WANT MAX SIMILARITY
                    #    maxSim=similarity_score
                    #    simWord=wordSelf 
        #Conclude if related
        if not ifConnected: #Not related, ie no connection with SelfConcept was above a fixed threshold.
            del self.graph[word] #delete entry from SelfGraph therefore
        else: # if related
            #Pick a word above threshold similarity:
            simWord=random.choice(possible_simWord)
            self.graph[word][0]=maxSim*self.graph[simWord][0] #adjust the weight of the node
        return ifConnected, simWord


    def extract(self, blabla, max_pick_word):
        """
            Extract wordnet nouns (or proper noun) from a blabla, which are not on the memory, nor on the selGraph, nor in EXCLUDED
            #TODO: TAKE ALSO WIKIPEDIA STILL
            Self Quest bounded to a maximum of max_pick_word to avoid too long wait. Beware, of found wikipediable word!
            Beware of upper or lower letters which can create conflicts.
            #TODO: Test Edge Cases and memor
        """
        OKWordnet=[]
        wn_lemmas = set(wordnet.all_lemma_names())#TODO: SHALL LOAD IT ONLY ONCE???
        if len(blabla)==0: #empty
            self.log.info("No new words to grow from.")
        else:
            counter=0#count words added
            for word, pos in nltk.pos_tag(word_tokenize(blabla)):
                if counter<max_pick_word:#Stop once has enough words
                    if pos in ['NN', 'NNS','NNP']:
                        if not word.isupper():#To avoid turning words like AI lower case. Else turn to lower case. Ex: donald_trump
                            word=word.lower()
                        #TODO: Need Lemmatizer to avoid words which have same roots?
                        if ((word in wn_lemmas) or (wikipedia.page(word).exists())) and not (word in OKWordnet):
                            if word in self.graph.keys():#Word is there, augment its weight.
                                self.graph[word][0]=self.graph[word][0]*1.1
                            else: #TODO: Shall exclude memory ?
                                OKWordnet.append(word)
                                counter+=1
            #Special case of duo words for wikipedia, such as global_warming https://en.wikipedia.org/wiki/Global_warming
            #TODO: FOR THESE, use wikipedia!?
            wordList=blabla.split()#then need word.strip(string.punctuation)
            token_list=nltk.pos_tag(word_tokenize(blabla))
            counter=0
            for token1, token2 in zip(token_list, token_list[1:]):#Consecutive token
                word1, pos1=token1
                word2, token2=token2
                if counter<max_pick_word and len(word1)>1 and len(word2)>1 and (word1 not in EXCLUDED) and (word2 not in EXCLUDED):#Stop once has enough words
                    if not word1.isupper(): #lower letter unless fully upper letter:check for proper noun
                        word1=word1.lower()
                    if not word2.isupper(): #lower letter unless fully upper letter
                        word2=word2.lower()
                    duo=word1+" "+word2
                    if wikipedia.page(duo).exists() and not (duo in OKWordnet):
                        if duo in self.graph.keys():
                            self.graph[duo][0]=self.graph[duo][0]*1.1
                        else:
                            OKWordnet.append(duo)
                            counter+=1
            self.log.info("New words to learn from")#+OKWordnet)
        return OKWordnet

#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#******************************************************SCRAPER PROCEDURES**************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************

    def surf_google(self,query, maximum_char_extract):
        """
        Main procedure to scrap google result of a query: will scrap the urls first, then the texts of the articles, parse the text and choose
        one of these extracts.
        #TODO: scraped data may contain all, while...?
            #TODO cut out symbols ~~~~ | ERROR    | 26872 | mycroft.enclosure.display_manager:_write_data:82 | Invalid control character at: line 1 column 33 (char 32)
        """
        #TODO: If none result satisfying criteria (length etc), relaunch further pages? OR TAKE SMALLER TEXT
         #TODO: TEST SCRAPPER

        ###(1) Scrap data from Google Search and get urls
        self.log.info("Scraping Google results and get urls")
        #BEFORE: 
        #data = self.google_search(query, api_key=my_api_key, cse_id=my_cse_id) #METHOD API OLD ONE
        #urls = self.get_urls(data)
        urls = self.retrieve_google_url(query)
        self.log.info("URLLLLLLLLLLLS" + str(urls))

        ###(2) Extract texts part
        self.log.info("Extracting the texts")
        scraped_data = self.parse_article_vytas(urls) 
        #BEFORE: self.parse_article(urls) #extract_text(urls, min_char_bit, min_char_block)
       
        ###(3) Choose one extract #NOTE: Discarded as for now only scrape one extract...
        # chosen_extract=self.choose_extract(scraped_data)
        self.log.info("Full Extract"+scraped_data)

        ###(4) Cut extract to max number, amd cut last part sentence
        final_extract=cut_extract(scraped_data, maximum_char_extract)#TODO: issue with cut extract for now with unconventional format
        #TODO: better take end article or beginning?
        self.log.info("Final Extract"+final_extract)
       

        return scraped_data, final_extract

    def retrieve_google_url(self, query, num_links=3):
        # query search terms on google
        # tld: top level domain, in our case "google.com"
        # lang: search language
        # num: how many links we should obtain
        # stop: after how many links to stop (needed otherwise keeps going?!)
        # pause: if needing multiple results, put at least '2' (2s) to avoid being blocked)
        online_search = []
        try:
            #online_search = search(query, stop=3)
            online_search = search(query, tld='com', lang='en', stop=num_links, num=num_links, pause=2)
        except:
            pass

        website_urls = []
        if online_search != None:
            for link in online_search:
                website_urls.append(link)
        # returns a list of links
        return website_urls

    def parse_article_vytas(self,urls):
        article_downloaded = False
        count=1
        while count<=4 and (not article_downloaded):
            try:
                # choose random url from list obtained from Google
                url = urls[random.randint(0, len(urls)-1)]
                # locate website
                article = newspaper.Article(url)
                # download website
                self.log.info('Try downloading ' + url)
                article.download()
                # parse .html to normal text
                article.parse()
                #get text
                content=article.text
                if len(content)>config["scraping_min_length"]:
                    article_downloaded = True
                    self.log.info("***Happy scraping! Beginning of extracted content:"+content[:config["scraping_min_length"]])
                else:
                    self.log.info("Unhappy scraping! Too short")
                count+=1
            except:
                self.log.info("ARTICLE DOWNLOAD FAILED")
                continue
        
        # article.nlp() # analyze text with natural language processing
        #can return summary too>>>#TODO: use summary?

        return content



#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#**********************************************************************LOAD PROCEDURES**************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************
#***********************************************************************************************************************************************

    def load_messages(self):
        self.log.info("Loading messages")
        path_folder=str(pathlib.Path(__file__).parent.absolute())+'/messages/'
        self.log.info(str(pathlib.Path(__file__).parent.absolute())+'/messages/message_askopinion.txt')
        self.MSG_ASKOPINION=load_data_txt("message_askopinion.txt", path_folder=path_folder)
        self.MSG_COMETOGETHER=load_data_txt("message_cometogether.txt",path_folder=path_folder)
        self.MSG_COMETOGETHER3=load_data_txt("message_cometogether3.txt", path_folder=path_folder)
        self.MSG_INTEREST=load_data_txt("message_interest.txt",path_folder=path_folder)
        self.MSG_NOINTEREST=load_data_txt("message_nointerest.txt", path_folder=path_folder)
        self.MSG_SHARE=load_data_txt("message_share.txt", path_folder=path_folder)
        self.MSG_NOTED=load_data_txt("message_noted.txt", path_folder=path_folder)
        self.MSG_LETMETHINK=load_data_txt("message_letmethink.txt", path_folder=path_folder)

    def save_data_file(self, filename, data, mode="w"):
        try:
            file_system = FileSystemAccess(str(self.skill_id))
            file = file_system.open(filename, mode)
            file.write(data)
            file.close()
            return True
        except Exception:#as e
            self.log.info("ERROR: could not save skill file " + filename)
            #LOG.error(e)
            return False

    def save_data_json(self, filename, data, mode="w"):
        try:
            file_system = FileSystemAccess(str(self.skill_id))
            file = file_system.open(filename, mode)
            json.dump(data, file)
            file.close()
            return True
        except Exception:
            self.log.info("ERROR: could not save skill file " + filename)
            return False

#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#**************************************************GPT2  GENERATION***************************************************************************************
#**************************************************************************************************************************************************
#************************************************************************************************************************************

    def gpt2_text_generation(self, context, length_output, temperature): 
        """
            One ML drift with gpt-2, with a context. Printed and said by VA.
            With some stochasticity
        """
        process = self.tokenizer.encode(context, return_tensors = "pt")
        generator = self.model.generate(process, max_length = length_output, temperature= temperature, repetition_penalty = 2.0, do_sample=True, top_k=20)
        drift = self.tokenizer.decode(generator.tolist()[0])
        
        return drift

#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#**************#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#************************************************************************************************************************************

    #the Skill creator must make sure the skill handler is removed when the Skill is shutdown by the system.
    def shutdown(self):
        """
            Remove this skill from list of fallback skills.
        """
        self.remove_fallback(self.handle_associative)
        super(AssociativeFallback, self).shutdown()

#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#***********************************************************************create SKILL***************************************************************************

def create_skill():
    return AssociativeFallback()

#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#************************************************************************************************************************************
#**************************************************************************************************************************************************
#**************************************************************************************************************************************************
#**********************************************************************ARCHIVE*************************************************************************

# def parse_one_article(self,url):
    #     text=""
    #     try:
    #         # locate website
    #         article = newspaper.Article(url)
    #         # download website
    #         self.log.info('Downloading ' + url)
    #         article.download()
    #         article_downloaded = True
    #         # parse .html to normal text
    #         article.parse()
    #         # analyze text with natural language processing
    #         #article.nlp()
    #         self.log.info("========Article Scraped==========================")
    #         #self.log.info(article.text)
    #         text = article.text
    #     except Exception: #requests.exceptions.RequestException:
    #         self.log.info("Article download failed.")
    #         pass
    #     return text

    # def parse_article(self,urls):
    #     """
    #     New procedure to extract text from articles.
    #     """
    #     articles=[]
    #     count=0
    #     for url in urls:
    #         if count<4:#only 4 working links
    #             self.log.info("article scraped n{}".format(count))
    #             try:
    #                 article=self.parse_one_article(url)
    #                 if article is not None and not article=="":
    #                     articles.append(article)
    #                     count+=1
    #             except:
    #                 continue

    #     return articles
